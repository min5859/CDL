import streamlit as st
from chatbot_base import ChatbotBase
from langchain_community.chat_models import ChatOllama

class CameraChatbotMistral7b(ChatbotBase):
    def __init__(self):
        super().__init__()
        # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ì •ì˜
        self.available_models = [
            "mistral:7b",
            "gemma2:latest",
            "llama3.2:latest",
        ]
        # ê¸°ë³¸ ì„¤ì •
        self.selected_model = "mistral:7b"
        self.temperature = 0.7  # ê¸°ë³¸ temperature ê°’
        self.llm = None
    
    def initialize_llm(self):
        """ì„ íƒëœ ëª¨ë¸ê³¼ temperatureë¡œ LLM ì´ˆê¸°í™”"""
        self.llm = ChatOllama(
            model=self.selected_model,
            temperature=self.temperature
        )

    def render(self):
        st.header("ğŸ’¬ OpenSource Chatbot")
        
        # Sidebarì— ëª¨ë¸ ì„¤ì • ì¶”ê°€
        with st.sidebar:
            st.header("Model Settings")
            
            # ëª¨ë¸ ì„ íƒ
            self.selected_model = st.selectbox(
                "Choose LLM Model",
                options=self.available_models,
                index=self.available_models.index(self.selected_model)
            )
            
            # Temperature ì¡°ì ˆ ìŠ¬ë¼ì´ë”
            self.temperature = st.slider(
                "Temperature",
                min_value=0.0,
                max_value=2.0,
                value=0.7,
                step=0.1,
                help="Higher values make the output more creative and diverse, lower values make it more focused and deterministic"
            )
            
            # Temperature ê°’ì— ë”°ë¥¸ ì„¤ëª… í‘œì‹œ
            temp_description = (
                "Conservative ğŸ¯" if self.temperature < 0.5
                else "Balanced âš–ï¸" if self.temperature < 1.0
                else "Creative ğŸ¨" if self.temperature < 1.5
                else "Experimental ğŸ”¬"
            )
            st.caption(f"Current Temperature Style: {temp_description}")
            
            # í˜„ì¬ ì„¤ì • ì •ë³´ í‘œì‹œ
            st.info(f"Model: {self.selected_model}\nTemperature: {self.temperature}")
            
            # ëª¨ë¸ì´ë‚˜ temperatureê°€ ë³€ê²½ë˜ë©´ LLM ì¬ì´ˆê¸°í™”
            settings_changed = (
                st.session_state.get('last_model') != self.selected_model or
                st.session_state.get('last_temperature') != self.temperature
            )
            
            if not self.llm or settings_changed:
                self.initialize_llm()
                st.session_state['last_model'] = self.selected_model
                st.session_state['last_temperature'] = self.temperature

        # íŒŒì¼ ì„ íƒ (ChatbotBaseì—ì„œ ì œê³µ)
        self.select_file()

        # ì„¸ì…˜ ìƒíƒœì— ë©”ì‹œì§€ ì €ì¥ì†Œê°€ ì—†ìœ¼ë©´ ì´ˆê¸°í™”
        self.initialize_messages()

        # ê¸°ì¡´ ëŒ€í™” ê¸°ë¡ì„ í™”ë©´ì— í‘œì‹œ
        self.display_messages()

        # ì‚¬ìš©ì ì§ˆë¬¸ ì…ë ¥ ì²˜ë¦¬
        if prompt := st.chat_input():
            # íŒŒì¼ ë‚´ìš©ì„ í¬í•¨í•œ ì§ˆë¬¸ ìƒì„± (ChatbotBaseì—ì„œ ì œê³µ)
            combined_prompt = self.prepare_combined_prompt(prompt)

            # ì§ˆë¬¸ì— ëŒ€í•œ ì‘ë‹µ ìƒì„± (íŒŒì¼ ë‚´ìš© ê¸°ë°˜)
            response = self.generate_response(combined_prompt)

            # ì‘ë‹µì„ í™”ë©´ì— í‘œì‹œí•˜ê³  ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€
            st.session_state["messages"].append({"role": "assistant", "content": response})
            st.chat_message("assistant").write(response)

    def generate_response(self, user_prompt):
        """
        ì„ íƒëœ Ollama ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
        Args:
            user_prompt: ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì§ˆë¬¸
        """
        try:
            response = self.llm.invoke(user_prompt)
            return response.content
        except Exception as e:
            return f"Error: {str(e)}"